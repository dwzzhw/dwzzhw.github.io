---
layout: post
title:  自适应停等限速方案介绍
date:   2019-01-31 14:00:00 +0800
categories: document
tag: 素材
---

* content
{:toc}


基于停止等待的自适应网络限速方案
====================================

本篇内容极简目录如下：
<br>一、限速背景
<br>二、原理与实现
<br>三、运行效果
<br>四、持续改进

一、网络限速背景
----------------------------------------------------------
移动开发时偶尔不经意间就会遇到预加载网络资源的需求，举两个栗子：

>场景1：后台偷偷的下载新版本更新包，就绪后嘚瑟的提示用户可以免流量立即安装更新了

![新安装包就绪]({{ '/styles/images/network-limit/pic_apk_ready.png' | prepend: site.baseurl  }})
<br>
<center>图1-1</center>

<br>

>场景2：赠送一个酷炫的动画道具，用户点击后立即弹出效果来

![点击动画道具]({{ '/styles/images/network-limit/pic_click_prop.png' | prepend: site.baseurl  }})
<br>
<center>图1-2</center>

<br>

类似这些场景下，就需要在后台先下载好相关数据。而设备带宽一般都有限，在存在后台请求占用大量带宽的情况下，因为资源竞争，势必会对正常的业务请求造成一定的干扰。极端情况下可能会出现莫名其妙的页面请求等待时间变长、视频播放频繁缓冲等现象。一旦出现这种后台的低优先级任务影响了前端页面展示的情况，就会得不偿失了。
因此，我们就有必要寻找一种以较低速度预加载资源的手段。

二、自适应限速实现
----------------------------------------------------------
先看一下当前网络上可以查到的主流流控方案，从不同的角度可以有不同的做法：

><b>服务端</b>    计数器、漏桶、令牌桶

><b>传输层</b>    滑动窗口

><b>客户端</b>    分片

><b>其他</b> 自定义协议如QUIC、KCP

<center>类似表格的表2-1</center>
<br>

服务端方案相当成熟，且不在移动客户端的控制范围，不讨论。其他自定义协议实现难度较大，使用范围较小，也不展开讨论。TCP层的滑动窗口是一个切入点，在我们后面会详细讨论，暂时按下不表。
目前客户端实现中比较好用的就是基于请求分片的做法了，其基本原理就是通过设置HTTP请求头里的range字段，每次请求一小段数据，然后动态调整分片请求间隔，达到限速的目的。在该实现的网络抓包图上，会展现处一系列离散的网络请求脉冲，该实现网络上已经有详细论述，本篇不再展开，其运行效果如下：


![分片限速效果图]({{ '/styles/images/network-limit/slice_throughput.png' | prepend: site.baseurl  }})
<br>
<center>图2-1</center>

<br>

行文至此，自然要问一句，有没有其他(更好的)方案呢？答案自然是有的，也就是本篇的主角--自适应的停止-等待限速方案。

该停等方案的出发点相当简单朴素，即：下载数据的过程无非就是读取网络数据字节流的过程，那么客户端主动降低处理这个字节流的速度，对应的下载速度不就是我们本地读取字节流的速度了吗？想象一个这么个场景，把网络字节流转存到一个原始时代的3.5寸软盘中，那么下载速度的上限也就是软盘IO的速度上限，1MB/min就不得了了。
知易行难，具体实现上还有不少困难需要克服，目前来看主要包括两点：

<br>

>1.自适应
顾名思义，自适应即是根据当前的可用网络速度按比例限制目标下载速度。如果仅仅需要把下载速度限制到一个固定的数值，so easy，此处略去不表。而要做到自适应，前提就是要先获取到当前的可用速度。

测速原理
要测量网络下载速度，只需要下载一定的数据，如200K，记录下下载用时，二者相除即可得到网络速度值。这里面隐藏这一个大问题，即网络下载的真实过程。整个过程如下图：

![停等测速原理]({{ '/styles/images/network-limit/stop_wait_theory.png' | prepend: site.baseurl  }})
<br>
<center>图2-2</center>
<br>

数据从网络服务器下发以后，并不是直接传递给应用程序进程，而是数据先从接收网卡读到系统的Kernal 缓存中，Kernal根据端口号将数据分配到各个APP所使用socket 的缓存中，此时即使用户进程还没有开始读取字节流，网络层也会持续将socket buffer写满。也就是说，用户程序中读取的字节流其实就是从这个socket buffer中传出来的。
对于我们的测速过程来说，尝试读取一段数据并计时的前提就要保证读取的数据是直接来自网络层的，而不是存在socket buffer中的内容。因此测速之前要先保证清空socket buffer。整个过程如下图所示：
![停等测速流程设计]({{ '/styles/images/network-limit/stop_wait_impl_arch.png' | prepend: site.baseurl  }})
<br>
<center>图2-3</center>
<br>

清空socket buffer并没有现成的API可用，我们这边的做法是从这个buffer字节流中持续尝试读取数据，每次4KB，然后判断实际读到的数据大小。当实际读到的数据块不足4KB时，即可认为buffer已经被清空，后续的内容全部直接来源于网络，然后就可以开始计算网络速度了。当然这里面隐含着一个前提就是APP读取buffer的速度要高于网络传输的速度。这个前提不成立的话，说明网络应该相当的好，限速的意义也就不大了，设置一个上限下载速度作为保底即可简单处理。
还有一点，因为网络一直是处在变化之中的，所以稳妥起见，前述的测速过程可以考虑周期性的执行一遍，以反映最新的网络情况，即上图中限速状态机在不同状态间不断循环的过程。

<br>

>2.网络接收窗口的影响

与第一个问题类似，在下载链接建立之后，app开始读取数据之前，因为socket buffer的存在，数据已然开始传输。实际上，这里提到的socket buffer其实就是TCP层的通知窗口。因为这个窗口的存在，对于我们的限速来说，开始这一段数据的传输是不可控的，填满网络接收窗口的过程是一个不可控的全速下载的过程，反映在数据抓包上就是开始的一个大脉冲，具体效果见下节贴图。

三、限速效果展示
----------------------------------------------------------
下面以下载一组动画资源为例对限速效果进行展示。 相关效果展示如下：
1. 测试数据整体下载情况：

![停等测速Demo描述]({{ '/styles/images/network-limit/stop_wait_demo_overview.png' | prepend: site.baseurl  }})
<br>
<center>图3-1</center>
<br>
图中展示的是连续下载6个MP4资源的整体网络请求情况。

2.测试过程网络包占用情况

![停等测速Demo描述]({{ '/styles/images/network-limit/stop_wait_demo_throughput.png' | prepend: site.baseurl  }})

<center>图3-2</center>
<br>

从这个图中可以清晰的看到6个文件的分界。在每个文件开始下载时，会首先出现一个较大的脉冲，此即上节第二个问题中描述的，开始正式下载前网络数据填满TCP接收窗口的过程。在这个脉冲之后，是一系列微小脉冲即低速下载组成的下载曲线。如果忽略起始的脉冲，后续过程的网络占用相对前面提到的分片限速方案来说要好的多，即网络占用更稳定。这种持续、低速、稳定的过程这个是我们所期望的。

3.测试过程TCP接收窗口变化情况

![停等测速Demo描述]({{ '/styles/images/network-limit/stop_wait_demo_receive_window.png' | prepend: site.baseurl  }})
<br>
<center>图3-3</center>
<br>

这张图反应的是该系列文件下载过程中TCP接收窗口的变化情况。留意一下时间戳，可以看到在网络曲线稳定后，接收窗口一直在0和较小的范围内波动。这其实也反映出了我们所采用停等限速方案的真正原理，即客户端接收窗口大小会持续的更新到服务端，当窗口变成0时，服务端会主动停止数据传输，当客户端通知服务端接收窗口非0之后，服务端又会继续之前的网络传输。
总结来说，停等方案正式基于通过控制本地接收窗口的可用空间大小来达到被动控制网络下载速度的目标。


四、持续改进
----------------------------------------------------------
前面详细讨论了自适应停等限速方案的原理及实现细节，过程中也提到了基于分片的限速方案。相对于分片方案来说，停等方案在首个大脉冲之后的网络曲线要平滑的多，且整个过程只需发起一次网络请求，相对于分片方案的每个分片一个请求，效率要明显提高，且不依赖于服务端range字段的支持，对是否采用长链接实现也没有要求。但是，很可惜，美中不足的是首个大脉冲太不美观。
<br><br>那么有办法消除或减小这个丑陋的首脉冲吗？答案当然是有的。
<br><br>前面的讨论中提到过，这个脉冲就是网络数据自发填满TCP接收窗口的过程，所以脉冲的大小取决于接收窗口的大小，而接收窗口的值与socket实现中传递进去的接收buffer的大小是正相关的，具体可以参考Socket实现中的SO_RCVBUF字段。
那么解决这个首脉冲的其中一个办法就是通过Socket的setReceiveBufferSize方法来改变接收窗口的大小，就可以按需调整首脉冲的高度了。
详细的实现过程就依赖于具体实现了。如果代码中可以直接拿到下载所用socket的引用，那么可以直接调用上述方法进行修改。以我们的下载模块为例，我们使用了OKHTTP的实现，在初始化时，OKHTTP提供了setSocketFactory这个方法，通过这个方法可以干预socket的创建过程，添加我们感兴趣的参数，比如减小socket的receiveBuffer。通过修改这个接收buffer就可以降低首脉冲了，改进后的效果见下图：

![停等测速Demo描述]({{ '/styles/images/network-limit/stop_wait_demo_throughput_2.png' | prepend: site.baseurl  }})
<br>
<center>图4-1</center>
<br>

与图3-2相比，改进后的首脉冲已经显著降低，可见通过修改Socket的接收buffer大小的方式是可行的。
当然上述方案前提是要有操作socket的途径，如果很不幸你的代码中还没有类似这种途径，这个方式是行不通的。这也是目前我们的实现中最大的一个局限，会在后续工作中尝试去解决它。

除了接收窗口之外，自适应的测速阶段其实也会产生首脉冲。这是因为测速是全速下载一定量数据的过程，测速完毕后网络仍然会继续填满剩余的接收窗口。于是这中间又会产生一个矛盾，即：若要测速更准确，就需要下载更长的数据段；若要减小首脉冲，则测速数据段越短越好。这个矛盾也需要根据实际场景来寻找最适合自己的测速数据长度。
